{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import dlib\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = (299, 299)  # Resize detected faces to this size\n",
    "MOTION_THRESHOLD = 20  # Lowered threshold for detecting motion between frames\n",
    "FRAME_SKIP = 2        # Reduced frame skip to ensure more frames are processed\n",
    "no_of_frames = 10\n",
    "\n",
    "# Initialize Dlib's frontal face detector (can switch to Haar cascades, MTCNN, or other)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "def extract_faces_from_frame(frame, detector):\n",
    "    \"\"\"\n",
    "    Detects faces in a frame and returns the resized faces.\n",
    "\n",
    "    Parameters:\n",
    "    - frame: The video frame to process.\n",
    "    - detector: Dlib face detector.\n",
    "\n",
    "    Returns:\n",
    "    - resized_faces (list): List of resized faces detected in the frame.\n",
    "    \"\"\"\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray_frame)\n",
    "    resized_faces = []\n",
    "\n",
    "    for face in faces:\n",
    "        x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()\n",
    "        crop_img = frame[y1:y2, x1:x2]\n",
    "        if crop_img.size != 0:  # Ensure cropped image is valid\n",
    "            resized_face = cv2.resize(crop_img, IMG_SIZE)\n",
    "            resized_faces.append(resized_face)\n",
    "\n",
    "    # Debug: Log the number of faces detected\n",
    "    #print(f\"Detected {len(resized_faces)} faces in current frame\")\n",
    "    return resized_faces\n",
    "\n",
    "def process_frame(video_path, detector, frame_skip):\n",
    "    \"\"\"\n",
    "    Processes frames to extract motion and face data concurrently.\n",
    "\n",
    "    Parameters:\n",
    "    - cap: OpenCV VideoCapture object.\n",
    "    - detector: Dlib face detector.\n",
    "    - frame_skip (int): Number of frames to skip for processing.\n",
    "\n",
    "    Returns:\n",
    "    - motion_frames (list): List of motion-based face images.\n",
    "    - all_faces (list): List of all detected faces for fallback.\n",
    "    \"\"\"\n",
    "    prev_frame = None\n",
    "    frame_count = 0\n",
    "    motion_frames = []\n",
    "    all_faces = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Skip frames to improve processing speed\n",
    "        if frame_count % frame_skip != 0:\n",
    "            frame_count += 1\n",
    "            continue\n",
    "\n",
    "        # Debug: Log frame number being processed\n",
    "        #print(f\"Processing frame {frame_count}\")\n",
    "\n",
    "        # # Resize frame to reduce processing time (optional, adjust size as needed)\n",
    "        # frame = cv2.resize(frame, (640, 360))\n",
    "\n",
    "        # Extract faces from the current frame\n",
    "        faces = extract_faces_from_frame(frame, detector)\n",
    "        all_faces.extend(faces)  # Store all faces detected, including non-motion\n",
    "\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if prev_frame is None:\n",
    "            prev_frame = gray_frame\n",
    "            frame_count += 1\n",
    "            continue\n",
    "\n",
    "        # Calculate frame difference to detect motion\n",
    "        frame_diff = cv2.absdiff(prev_frame, gray_frame)\n",
    "        motion_score = np.sum(frame_diff)\n",
    "\n",
    "        # Debug: Log the motion score\n",
    "        #print(f\"Motion score: {motion_score}\")\n",
    "\n",
    "        # Check if motion is above the defined threshold and add the face to motion frames\n",
    "        if motion_score > MOTION_THRESHOLD and faces:\n",
    "            motion_frames.extend(faces)\n",
    "\n",
    "        prev_frame = gray_frame\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return motion_frames, all_faces\n",
    "\n",
    "def select_well_distributed_frames(motion_frames, all_faces, no_of_frames):\n",
    "    \"\"\"\n",
    "    Selects well-distributed frames from the detected motion and fallback faces.\n",
    "\n",
    "    Parameters:\n",
    "    - motion_frames (list): List of frames with detected motion.\n",
    "    - all_faces (list): List of all detected faces.\n",
    "    - no_of_frames (int): Required number of frames.\n",
    "\n",
    "    Returns:\n",
    "    - final_frames (list): List of selected frames.\n",
    "    \"\"\"\n",
    "    # Case 1: Motion frames exceed the required number\n",
    "    if len(motion_frames) >= no_of_frames:\n",
    "        interval = len(motion_frames) // no_of_frames\n",
    "        distributed_motion_frames = [motion_frames[i * interval] for i in range(no_of_frames)]\n",
    "        return distributed_motion_frames\n",
    "\n",
    "    # Case 2: Motion frames are less than the required number\n",
    "    needed_frames = no_of_frames - len(motion_frames)\n",
    "\n",
    "    # If all frames together are still less than needed, return all frames available\n",
    "    if len(motion_frames) + len(all_faces) < no_of_frames:\n",
    "        #print(f\"Returning all available frames: {len(motion_frames) + len(all_faces)}\")\n",
    "        return motion_frames + all_faces\n",
    "\n",
    "    interval = max(1, len(all_faces) // needed_frames)\n",
    "    additional_faces = [all_faces[i * interval] for i in range(needed_frames)]\n",
    "\n",
    "    combined_frames = motion_frames + additional_faces\n",
    "    interval = max(1, len(combined_frames) // no_of_frames)\n",
    "    final_frames = [combined_frames[i * interval] for i in range(no_of_frames)]\n",
    "    return final_frames\n",
    "\n",
    "def extract_frames(no_of_frames, video_path):\n",
    "  motion_frames, all_faces = process_frame(video_path, detector, FRAME_SKIP)\n",
    "  final_frames = select_well_distributed_frames(motion_frames, all_faces, no_of_frames)\n",
    "  return final_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "MAX_FRAMES=no_of_frames\n",
    "def predict_video(model, video_path):\n",
    "    \"\"\"\n",
    "    Predict if a video is REAL or FAKE using the trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The loaded deepfake detection model.\n",
    "    - video_path: Path to the video file to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - str: 'REAL' or 'FAKE' based on the model's prediction.\n",
    "    \"\"\"\n",
    "    # Extract frames from the video\n",
    "    frames = extract_frames(no_of_frames, video_path)\n",
    "\n",
    "    # Convert the frames list to a 5D tensor (1, time_steps, height, width, channels)\n",
    "    if len(frames) < MAX_FRAMES:\n",
    "        # Pad with zero arrays to match MAX_FRAMES\n",
    "        while len(frames) < MAX_FRAMES:\n",
    "            frames.append(np.zeros((299, 299, 3), dtype=np.float32))\n",
    "\n",
    "    frames = frames[:MAX_FRAMES]  # Ensure it only has MAX_FRAMES\n",
    "    frames = np.array(frames)     # Convert to a numpy array\n",
    "    frames = preprocess_input(frames)  # Preprocess frames for InceptionV3\n",
    "\n",
    "    # Expand dims to fit the model input shape\n",
    "    input_data = np.expand_dims(frames, axis=0)  # Shape becomes (1, MAX_FRAMES, 299, 299, 3)\n",
    "\n",
    "    # Predict using the model\n",
    "    prediction = model.predict(input_data)\n",
    "    probability = prediction[0][0]  # Get the probability for the first (and only) sample\n",
    "    # Convert probability to class label\n",
    "    predicted_label = 'FAKE' if probability >= 0.5 else 'REAL'\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the model\n",
    "model_path = 'deepfake_detection_model.h5'\n",
    "model = tf.keras.models.load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "The video is predicted to be: FAKE\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "video_path = 'Dataset/train_sample_videos/abofeumbvv.mp4'  \n",
    "result = predict_video(model, video_path)\n",
    "print(f'The video is predicted to be: {result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
